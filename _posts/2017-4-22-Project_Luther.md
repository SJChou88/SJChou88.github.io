---
layout: post
title: Project 2 - Luther  
---

For the second project at Metis bootcamp, I chose to do a data scrapping and linear regression model on soccer data. This is a subject I'm particularly interested in and felt I would be able to learn a lot about data scrapping and apply the regression techniques we were learning about in class to the dataset well. My original goal was to attempt to predict a teams success in a given season using previous seasons data and transfers done, which would give an additional benefit of measuring how much money had to be spent to purchase an additional point.
The website I chose to scrap is <www.transfermarkt.com>. This is a german website that is well known for having a large transfer database for many teams. I was originally going to scrape another website for the tables of each previous season, but transfermarkt actually had a pretty good source for that as well. Sample website that I would need to scrape look like below.
![alt text](https://github.com/SJChou88/SJChou88.github.io/blob/master/images/soccer_analysis_images/Standings_Image.png?raw=true "Table Image")
![alt text](https://github.com/SJChou88/SJChou88.github.io/blob/master/images/soccer_analysis_images/Transfers_Image.png?raw=true "Transfer Image")

Since I would have to go through multiple seasons (chose to use 25 since the premier league began in 92/93) and click through pages from one another, I decided to use scrappy and coded this in spyder (designed my spiders in spyder!). From the season tables page to the transfers page, I would have to click on each team in the table and than navigate to the transfers all tab, before finally scraping each table from the transfers page. Navigating through the 25 seasons was simple, as the website had a standard naming convention for cycling each year. Scrappy was very intuitive to use with the scrappy shell up. It was simple to test out xpath code in order to figure out how to navigate around a page. Inspecting an element on a page and writing out the scrappy code to extract that data became very quick. There was an annoying moment when I was merging the data later where I realized the naming convention for teams was different between the season standings page and the team's own transfer page, but I figured out the logo for the tables page had an alt text that served to be the standardized long form name of each team. For some odd reason, I could not export the data in terms of a csv and the JSON export function required a single string as a key. I ended up constructing a key out of multiple items, which end up being dropped when being constructed into a pandas dataframe later on.

After scraping the data, some code had to be put in place for generating a dataset to model on. The season standings was pretty simple. Most of the work was in formating the data into numbers and generating a season value. Goals for and against were shifted to the next year, and I also created a 'CalcP' column in my dataset, as some old seasons used wins equaling to 2 points, rather than the current standard of 3 points for a win and 1 point for a draw. For transfer information, more data construction had to be done. Scraping yielded two datasets for each season, one for arrivals and one for departures. I had to write code to count each category (players in/out on loan, frees, or paid for with a fee) as well as totally the fees. The fees had a standard form of "nn,nn Mill./Th." and a euro sign. Breaking this out, I was able to construct a number with some string replacement on the first element to create a more standardized form, a multiplicative element from the second, and simply dropping the last euro sign. With a groupby and sum / count, this dataset was complete and left merged onto the season standings to create our modeling dataset, dropping all the extra transfer data that we did not require.
The modeling portion was actully fairly simple. I constructed a few interative models in order to see how each feature affected the model. I first created a "future" model, using goals for and against of the current season in order to verify that goals are a good predictor of how good a team is. This was apparent with a very high r-squared value of .875. This was quickly followed with a model using previous goals for/against and a categorical value for leagues (leagues have reputations for playing differently). The r-squared dropped to below .5, but I was still optimistic. Adding in the transfer information here had some very cool results.
![alt text](https://github.com/SJChou88/SJChou88.github.io/blob/master/images/soccer_analysis_images/Regression_Image.png?raw=true "Regression Image")

This implies that 8 million euros can buy you a point! It also implies there is a penalty for players coming in, which is offset by the amount spent on a player (expensive players improve the team on average!). There are a few p-values that are not significant, mainly departure numbers. Free players departing have a very small coefficient as well as a large p-value, implying they have little affect on this model. I tested out some feature selection methods as well as lasso, but they ended up removing the same features that were not significant. With that, we have our final model which had a tested r-squared around .5. Not great, but I believe the most interesting part was the euro value associated with a point, which is pretty cool. I generated a list of the largest outliers from my model and they were all big surprises in those seasons. You can see the list in the presentation on my [github](https://github.com/SJChou88/Soccer_Analysis) on  for this project. All in all, a fun project that taught me how to scrape web data and apply some linear modeling techniques.
