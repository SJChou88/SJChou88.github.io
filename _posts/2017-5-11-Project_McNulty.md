---
layout: post
title: Project 3 - McNulty  
---

For the third project at Metis bootcamp, I chose to do a data scrapping and classification model on the NFL draft. I came up with this idea from watching the NFL draft, wondering if it was possible to utilize machine learning to predict first round draft picks. It seemed feasible but I ended up with a number of concerns which will be mentioned as we proceed through this post. A brief google search actually revealed that someone came up with a similar project, [albeit in R](https://seanjtaylor.github.io/learning-the-draft/). I ended up taking a similar approach, although I diverged on a few aspects for reasons I’ll get into. 
I originally tested my methods out on Sean Taylor’s data, but I ended up scraping [sports-reference](http://www.sports-reference.com/cfb/) myself for the additional 2 years of data as well as some noted errors in his data. An example is Duke Johnson’s statistics being recorded incorrectly with receiving statistics flipped with rushing statistics. The data scraped from sports-reference include draft history, combine data, and college statistics for each player. Another website I used was the [collegepollarchive](http://collegepollarchive.com/football/ap/seasons.cfm?seasonid=2016) to obtain college rankings at the end of the year. This will eventually be used as a proxy for team success and player visibility.  
Scraping the websites was pretty similar to the previous project using scrapy. There were a some data issues in several of the datasets. Some players did not participate in every drill, leaving null values in several places. I ended up imputing the data by applying the median of the stat by position. The college statistics websites for each player differed based on position with table order being different. There was also obvious missing values for players such as number of games missing. This could lead to some poor modeling for certain players, but overall should average out.  
Modeling the data was much harder than anticipated. This is a basic classification problem, but with an unbalanced dataset. I tested out a variety of methods, such as SVC, random forest, and naive bales, but ended up going with XGBoost. This method has won many kaggle competitions and is known for it’s speed, which was useful for my testing of many different parameters. I also wanted to get experience with the usage. XGBoost had built in parameters to take care of the unbalanced classes as well. I had attempted some techniques such as downsampling and using the imblearn package, but nothing worked particularly well. XGBoost with a grid search seemed to perform better but I still had more thoughts to try out that may have done better by the deadline.  
The end pipeline that I ended up using was a logistic regression on the normalized data to obtain important features and then running xgboost on the remainder. Some physical characteristics are known to be more important for certain positions, so an interaction term was created between positions (a categorical variable) and the combine statistics. I also initially used colleges as a categorical variable, but ended up switching to a binomial “ranked in the top 25” at the end of a college season as a proxy for team success and popularity. 
The final xgboost model had a grid search performed on it using cohen’s kappa. Traditional scores such as accuracy, precision, and recall did not grant the kind of results we were looking for. With unbalanced classes, accuracy was leading to a result biased towards predicting every player to not be a first round pick. Maximizing recall towards first rounders had an almost opposite problem. F1 was briefly considered but the preliminary results were lacking. Cohen’s kappa attempts to maximize the agreement between the classifier and the ground truth. This combined with logloss reduction should have given the best possible model. The traditional classification errors metrics did not perform great with the traditional train/test split. The real cool application of this model would be attempting to predict the 2017 draft. I did this by holding out the 2017 data as a predict set, using 2000 - 2011 as a train set, and 2012-2016 as a test set. The traditional metrics were the same, but for 2017, I would simply order the players by their probabilities of being a first round pick and then taking the top 32 as a first rounder. I ended up predicting about 10 of the 32 correctly. Not too bad but room for future improvement.  
![alt text](https://github.com/SJChou88/SJChou88.github.io/images/NFL_Draft/raw/master/src/common/images/2017_top_10.png “Top 10 of 2017 vs Predicted“)
### Items for the future  
  * Perform more feature selection - Some features that were eliminated may have shed more light on certain positions as well as the opposite, some included features causing other features to have poor performance
  * Training on players picked in the top 20 - These players are more likely to have had the actual skill to be in the first round and not have been picked based off of team needs  
  * Building models for each position separately - Model each position separately, and then creating an aggregate model. This would hopefully classify players better and be more invariant on team needs.  
  * Use random search for quicker parameter search - this is self explanatory. Felt as though I ran out of time a bit  
  * Building a model for players who are in the league after 4 years - This would identify the features that allow a player to have a longer than average career ( typical career is 3.3 years ).  
Here’s a link to the GitHub repo : [link](https://github.com/SJChou88/NFL_Draft)